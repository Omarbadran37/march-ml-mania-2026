# Prediction Strategies & Model Playbook

A progressive guide from the simplest one-liner approach all the way to competitive ML models.
Everything here can be generated by your AI or coding agent. You don't need to write the code
yourself — you just need to understand what you're asking for and why.

---

## The Prediction Ladder

| Approach | Difficulty | Best for | Expected Brier Score | Time |
|----------|-----------|----------|---------------------|------|
| Historical seed win rates | ⭐ Beginner | Anyone | ~0.220–0.230 | 30 min |
| Elo rating system | ⭐⭐ Easy | Path B/C | ~0.205–0.215 | 1–2 hrs |
| Logistic regression on features | ⭐⭐⭐ Intermediate | Path C | ~0.195–0.210 | 2–4 hrs |
| XGBoost / gradient boosting | ⭐⭐⭐⭐ Advanced | Path C | ~0.185–0.200 | 4–8 hrs |
| Ensemble + calibration | ⭐⭐⭐⭐⭐ Expert | Path C | ~0.170–0.190 | 8+ hrs |

Start at the top and work your way down. Each step builds on the previous one.

---

## Approach 1 — Historical Seed Win Rates

**What it is:** Look at every tournament game in history. For each seed vs. seed matchup (e.g., #1 vs. #16, #5 vs. #12), calculate the historical win rate. Use those win rates as your predictions.

**Why it works:** Seeds encode a lot of information — the selection committee is reasonably good at ranking teams. Historical rates give you calibrated probabilities for free.

**Prompt to generate it:**
```
Using MNCAATourneySeeds.csv and MTourneyResults.csv, calculate the historical win rate for every possible seed vs. seed matchup (e.g., #1 seed vs. #16 seed, #5 vs. #12, etc.) using all available years. Then, using the 2026 seeds and SampleSubmission.csv, generate a submission file where each game's probability is the historical win rate for that seed matchup. Output a CSV with columns ID and Pred. Clip all probabilities to [0.001, 0.999].
```

---

## Approach 2 — Elo Rating System

**What it is:** A dynamic rating system (used in chess and many sports) where teams gain or lose rating points based on wins/losses and the strength of their opponent. Teams that beat strong opponents gain more points than teams that beat weak ones.

**Why it works:** Elo captures momentum and strength of schedule better than seeds alone. It updates continuously as games are played.

**Prompt to generate it:**
```
Implement an Elo rating system for NCAA basketball using MRegularSeasonResults.csv. Start all teams at 1500. For each game, update ratings using the standard Elo formula with K=20. Reset ratings 25% toward 1500 each season to account for roster changes. At the end of the 2026 regular season, use the final Elo ratings to predict tournament game probabilities using the Elo win probability formula: P(A beats B) = 1 / (1 + 10^((Elo_B - Elo_A) / 400)). Generate a submission CSV.
```

---

## Approach 3 — Logistic Regression

**What it is:** A statistical model that learns which team features (point differential, win rate, strength of schedule, seed) best predict tournament outcomes. It gives you a probability output directly.

**Why it works:** Logistic regression is well-calibrated by default, fast to train, and interpretable — you can see exactly which features matter most.

**Prompt to generate it:**
```
Build a logistic regression model to predict NCAA tournament outcomes. Features should include: seed difference, average point differential in regular season, win percentage, strength of schedule (average opponent win %), home/away point differential, and conference win rate. Train on all historical tournament games. Use cross-validation and report the Brier Score on holdout data. Generate a 2026 submission CSV.
```

---

## Approach 4 — XGBoost / Gradient Boosting

**What it is:** A powerful machine learning algorithm that builds hundreds of small decision trees and combines them. It often outperforms logistic regression on structured/tabular data like this.

**Why it works:** XGBoost captures non-linear relationships between features that logistic regression misses. For example, it can learn that a #12 seed from a power conference is much more dangerous than a #12 seed from a mid-major.

**Prompt to generate it:**
```
Replace the logistic regression with an XGBoost classifier. Use the same feature set plus: tournament appearances in last 5 years, tournament wins in last 5 years, coach tournament win rate, and average Elo rating over the last 10 games of the regular season. Tune max_depth (3–7), n_estimators (100–500), and learning_rate (0.01–0.1) using cross-validation. Report Brier Score vs. logistic regression baseline. Generate a 2026 submission CSV.
```

---

## Approach 5 — Ensemble + Calibration

**What it is:** Combine multiple models (e.g., seed win rates + Elo + logistic regression + XGBoost) by averaging their predictions. Then apply probability calibration to correct for systematic over- or under-confidence.

**Why it works:** No single model is perfect. Ensembling reduces variance. Calibration directly optimizes for the Brier Score by making sure your 60% predictions win about 60% of the time.

**Prompt to generate it:**
```
Build an ensemble by averaging predictions from: (1) historical seed win rates, (2) Elo-based probabilities, (3) logistic regression, and (4) XGBoost. After averaging, apply Platt scaling to calibrate the ensemble probabilities. Show me a reliability diagram (calibration curve) before and after calibration. Report the Brier Score for each individual model and the final ensemble. Generate the final submission CSV.
```

---

## Data Exploration Prompts

Use these prompts in any AI tool — Cowork or your AI coding agent of choice — to understand the data before you build anything. Good exploration catches problems early and reveals which features are worth building.

**Explore 1 — Data overview:**
```
Load all the March ML Mania CSV files and give me a structured summary: file names, row counts, column names, date ranges covered, and any obvious data quality issues (missing values, duplicate rows, inconsistent team IDs). Highlight anything that might cause problems when I join tables.
```

**Explore 2 — Seed analysis:**
```
Using the last 15 years of tournament data, show me: (1) win rate by seed in each round (1st round through championship), (2) the most common upset matchups, (3) which conferences produce the most upsets, and (4) whether home region or travel distance correlates with performance. Show results as tables and charts.
```

**Explore 3 — Team strength signals:**
```
For the 2026 regular season, identify the top 20 teams by each of these metrics: overall win percentage, point differential per game, strength of schedule, and wins against top-25 opponents. Where do the metrics agree? Where do they disagree? Which teams might be over- or under-seeded based on these signals?
```

**Explore 4 — Feature correlation:**
```
Create a correlation matrix showing how strongly each potential feature (seed, win rate, point differential, SOS, Elo, conference rank) correlates with tournament outcomes. Which features have the strongest signal? Which are redundant? Use this to recommend which 5–8 features to prioritize in a model.
```

**Explore 5 — Historical calibration check:**
```
Take a simple model: for each team pair, predict win probability using only seed difference. Plot a reliability diagram: does a team predicted to win with 70% probability actually win about 70% of the time historically? Where is the model most miscalibrated? What would calibration correction do to the Brier Score?
```

---

## Feature Engineering Prompts

**Feature 1 — Core team stats:**
```
From MRegularSeasonResults.csv, create a summary table for the 2026 season with one row per team. Include: games played, wins, losses, win percentage, average points scored, average points allowed, average point differential, home win rate, and away win rate.
```

**Feature 2 — Strength of schedule:**
```
Calculate strength of schedule for every 2026 team. Method: for each team, average the win percentages of all their opponents. Teams with harder schedules get credit. Join this with the regular season summary table and show me the top 20 teams by adjusted win rate (win rate adjusted for SOS).
```

**Feature 3 — Late-season momentum:**
```
Calculate each team's win rate and average point differential in just their last 10 regular season games of 2026. Compare this to their full-season stats. Which teams are peaking? Which are in a slump? Add these as momentum features to the team summary table.
```

**Feature 4 — Historical tournament experience:**
```
Create features capturing each 2026 team's historical tournament performance: tournament appearances in the last 5 years, average seed in those appearances, total tournament wins, wins in the round of 32 or later, and whether they have a coach with significant tournament experience. Join to the main features table.
```

**Feature 5 — Conference strength:**
```
Calculate conference-level strength signals for 2026: average Elo rating of conference members, conference tournament champion performance in recent NCAAs, and inter-conference win rates. Add a conference tier label (Power 6, Mid-Major, etc.) and a conference strength score to each team's features.
```

---

## Model Building & Evaluation Prompts

**Model 1 — Baseline and benchmark:**
```
Train a logistic regression model on historical tournament games (2010–2025) to predict win probability. Features: seed difference, point differential difference, SOS difference, win rate difference. Use 5-fold cross-validation. Report: Brier Score (mean ± std), accuracy, and ROC AUC. Compare to the naive baseline (always predict 50%).
```

**Model 2 — Feature importance:**
```
After training the model, show me: (1) the coefficient for each feature and what direction it predicts, (2) which features are most predictive, (3) any features that seem to hurt performance (negative importance). Based on this, which 3 features should I focus on improving next?
```

**Model 3 — XGBoost upgrade:**
```
Train an XGBoost model with the same feature set. Use grid search over: max_depth [3, 5, 7], n_estimators [100, 300, 500], learning_rate [0.01, 0.05, 0.1], subsample [0.7, 1.0]. Use 5-fold cross-validation. Report Brier Score vs. logistic regression. Show the top 10 most important features by XGBoost's feature importance score.
```

**Model 4 — Calibration:**
```
Take my XGBoost model's raw probability outputs and apply Platt scaling calibration. Show me: (1) a reliability diagram before calibration, (2) a reliability diagram after calibration, (3) the Brier Score before and after, (4) a histogram of predicted probabilities before and after. Explain in plain English what the calibration is correcting.
```

**Model 5 — Final submission:**
```
Using the best model (calibrated XGBoost), generate the final 2026 submission CSV. For every row in SampleSubmission.csv, compute the win probability for Team1 vs. Team2 using our engineered features. Clip all predictions to [0.001, 0.999]. Validate: check that all IDs from SampleSubmission.csv are present, no nulls, correct format. Show the first 10 rows and the probability distribution.
```

---

## End-to-End Session Script

If you want to hand your AI coding agent (or Cowork) the entire pipeline in one session, use this sequence. Each prompt builds on the previous one. Copy them in order.

1. **Set up:** "Load all March ML Mania CSVs and confirm the file structure. Tell me what years are available and any data quality issues."
2. **Explore:** "Run an EDA: distribution of seeds, historical win rates by seed, upset frequency by round."
3. **Feature build:** "Create a features table for 2026: seeds, win rate, point differential, SOS, Elo, momentum (last 10 games), tournament experience."
4. **Train:** "Train logistic regression + XGBoost on historical tournament games. Report Brier Score for each with cross-validation."
5. **Calibrate:** "Apply Platt scaling to the XGBoost outputs. Show calibration curve before/after."
6. **Submit:** "Generate the final submission CSV, validate format against SampleSubmission.csv, and show me the probability distribution."
